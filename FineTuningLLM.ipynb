{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning LLMs\n",
    "\n",
    "Fine-tuning in machine learning is the process of adapting a pre-trained model for specific tasks or use cases. It could be considered a subset of the broader technique of transfer learning: the practice of leveraging knowledge an existing model has already learned as the starting point for learning new tasks.\n",
    "\n",
    "Fine-tuning is an essential part of the LLM development cycle, allowing the raw linguistic capabilities of base foundation models to be adapted for a variety of use cases, from chatbots to coding to other domains both creative and technical.\n",
    "\n",
    "By fine-tuning a model on a small dataset of task-specific data, you can improve its performance on that task while preserving its general language knowledge.\n",
    "\n",
    "Fine-tuning LLMs involves adapting a pre-trained model to a specific domain or task by training it further on a domain-specific dataset. This involves multiple steps, from preparing the dataset to implementing fine-tuning strategies.\n",
    "\n",
    "#### **1. Preparing the Dataset**\n",
    "The dataset is critical for fine-tuning as it determines the specificity and performance of the model on your desired tasks. Data preparation involves curating and preprocessing the dataset to ensure its relevance and quality for the specific task. This may include tasks such as cleaning the data, handling missing values, and formatting the text to align with the model's input requirements.\n",
    "\n",
    "##### Steps:\n",
    "a.\t**Data Collection**: Collect domain-specific data relevant to your task\n",
    "\n",
    "b.\t**Data Cleaning**: Remove duplicates, irrelevant information, or noise and ensure proper formatting\n",
    "\n",
    "c.\t**Data Formatting for Training**: Use appropriate input-output formats and split data into training, validation, and testing sets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Split dataset\n",
    "train, temp = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save splits\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Choosing the right pre-trained model**\n",
    "\n",
    "It’s crucial to select a pre-trained model that aligns with the specific requirements of the target task or domain. Understanding the architecture, input/output specifications, and layers of the pre-trained model is essential for seamless integration into the fine-tuning workflow.\n",
    "\n",
    "Factors such as the model size, training data, and performance on relevant tasks should be considered when making this choice. By selecting a pre-trained model that closely matches the characteristics of the target task, you can streamline the fine-tuning process and maximize the model's adaptability and effectiveness for the intended application.\n",
    "\n",
    "If your model benefits from domain-specific knowledge not captured by the base model, pre-train it with unsupervised learning on large datasets.\n",
    "\n",
    "##### Steps:\n",
    "\n",
    "* Use language modeling tasks like Causal Language Modeling (CLM) or Masked Language Modeling (MLM).\n",
    "\n",
    "* Tokenize the dataset and pass it through the model without labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Identifying the right parameters for fine-tuning**\n",
    "\n",
    "Configuring the fine-tuning parameters is crucial for achieving optimal performance in the fine-tuning process. Parameters such as the learning rate, number of training epochs, and batch size play a significant role in determining how the model adapts to the new task-specific data. Additionally, selectively freezing certain layers (typically the earlier ones) while training the final layers is a common practice to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Fine-Tuning**\n",
    "\n",
    "Fine-tuning adapts the model to a specific downstream task using labeled data.\n",
    "\n",
    "##### Steps\n",
    "\n",
    "* Use task-specific loss functions (e.g., CrossEntropyLoss for classification)\n",
    "* Employ frameworks like Hugging Face, PyTorch Lightning, or TensorFlow.\n",
    "\n",
    "Example (Text Classification with Hugging Face):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize dataset\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_data = dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Validation/Evaluating**\n",
    "\n",
    "Validation involves evaluating a fine-tuned model’s performance using a validation set. Monitoring metrics such as accuracy, loss, precision, and recall provide insights into the model's effectiveness and generalization capabilities.\n",
    "\n",
    "By assessing these metrics, you can gauge how well the fine-tuned model is performing on the task-specific data and identify potential areas for improvement. This validation process allows for the refinement of fine-tuning parameters and model architecture, ultimately leading to an optimized model that excels in generating accurate outputs for the intended application.\n",
    "\n",
    "Evaluate the fine-tuned model on the test set using appropriate metrics:\n",
    "\n",
    "* **Text Classification**: Accuracy, F1-Score.\n",
    "* **Summarization**: ROUGE.\n",
    "* **Question Answering**: F1, Exact Match (EM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Load metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# Evaluate predictions\n",
    "predictions = trainer.predict(tokenized_data[\"test\"])\n",
    "accuracy = metric.compute(predictions=predictions.predictions.argmax(-1), references=predictions.label_ids)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Model iteration**\n",
    "\n",
    "Model iteration allows you to refine the model based on evaluation results. Upon assessing the model's performance, adjustments to fine-tuning parameters, such as learning rate, batch size, or the extent of layer freezing, can be made to enhance the model's effectiveness.\n",
    "\n",
    "Additionally, exploring different strategies, such as employing regularization techniques or adjusting the model architecture, enables you to improve the model's performance iteratively. This empowers engineers to fine-tune the model in a targeted manner, gradually refining its capabilities until the desired level of performance is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Model deployment**\n",
    "\n",
    "Model deployment marks the transition from development to practical application, and it involves the integration of the fine-tuned model into the specific environment. This process encompasses considerations such as the hardware and software requirements of the deployment environment and model integration into existing systems or applications.\n",
    "\n",
    "Additionally, aspects like scalability, real-time performance, and security measures must be addressed to ensure a seamless and reliable deployment. By successfully deploying the fine-tuned model into the specific environment, you can leverage its enhanced capabilities to address real-world challenges.\n",
    "\n",
    "Save the model and deploy it using APIs like Hugging Face’s transformers pipeline or FastAPI for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load fine-tuned model\n",
    "model_pipeline = pipeline(\"text-classification\", model=\"./results\")\n",
    "\n",
    "# Inference\n",
    "result = model_pipeline(\"This movie was fantastic!\")\n",
    "print(result)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
